{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "from collections import namedtuple, deque\n",
    "import math\n",
    "import random\n",
    "import logging\n",
    "\n",
    "import tqdm\n",
    "import tqdm.notebook as tqdm_notebook\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import IterableDataset\n",
    "from torchmetrics import MeanMetric\n",
    "\n",
    "from lightning.pytorch import LightningModule, loggers as pl_loggers\n",
    "from lightning.pytorch.trainer import Trainer\n",
    "from lightning.pytorch import seed_everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('lightning.pytorch')\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Experience = namedtuple('Experience', ['observations', 'actions', 'values', 'returns', 'advantages', 'log_probs'])\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, capacity: int = 1000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, sample_size: int = 1):\n",
    "        replace = False if sample_size < len(self) else True\n",
    "        indices = np.random.choice(len(self), sample_size, replace=replace)\n",
    "        # collate experiences\n",
    "        for idx in indices:\n",
    "            yield {key: getattr(self.buffer[idx], key) for key in Experience._fields}\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "\n",
    "\n",
    "class RLDataset(IterableDataset):\n",
    "\n",
    "    def __init__(self, buffer: ReplayBuffer, sample_step_num: int = 1):\n",
    "        self.buffer = buffer\n",
    "        self.sample_step_num = sample_step_num\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.sample_step_num\n",
    "\n",
    "    def __iter__(self):\n",
    "        for data in self.buffer.sample(self.sample_step_num):\n",
    "            yield data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_loss(advantages: torch.Tensor, ratio: torch.Tensor, cliip_coef: float):\n",
    "    p_loss1 = -advantages * ratio\n",
    "    p_loss2 = -advantages * torch.clamp(ratio, 1 - cliip_coef, 1 + cliip_coef)\n",
    "    return torch.max(p_loss1, p_loss2).mean()\n",
    "\n",
    "\n",
    "def value_loss(new_values: torch.Tensor, old_values: torch.Tensor, returns: torch.Tensor, clip_coef: float,\n",
    "               clip_vloss: bool, vf_coef: float):\n",
    "    new_values = new_values.view(-1)\n",
    "    if not clip_vloss:\n",
    "        values_pred = new_values\n",
    "    else:\n",
    "        values_pred = old_values + torch.clamp(new_values - old_values, -clip_coef, clip_coef)\n",
    "    return vf_coef * F.mse_loss(values_pred, returns)\n",
    "\n",
    "\n",
    "def entropy_loss(entropy: torch.Tensor, ent_coef: float):\n",
    "    return -entropy.mean() * ent_coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOModuleBase(nn.Module):\n",
    "\n",
    "    def __init__(self, mlp_cfg: dict, inp_channels: int, out_channels: int, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = self._create_mlp(mlp_cfg, inp_channels, out_channels)\n",
    "\n",
    "    def _create_mlp(self, mlp_cfg, inp_channels, out_channels):\n",
    "        channels = [inp_channels] + mlp_cfg['channels'] + [out_channels]\n",
    "        use_layer_norm = mlp_cfg.get('use_layer_norm', False)\n",
    "        act_func = mlp_cfg.get('act_func', 'ReLU')\n",
    "\n",
    "        _mlp = nn.Sequential(*[\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_chn, out_chn, bias=True),\n",
    "                nn.LayerNorm(out_chn) if use_layer_norm else nn.Identity(),\n",
    "                getattr(nn, act_func)())\n",
    "            for in_chn, out_chn in zip(channels[:-2], channels[1:-1])\n",
    "        ])\n",
    "        _mlp.append(nn.Linear(channels[-2], channels[-1], bias=True))\n",
    "        return _mlp\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class PPOActor(PPOModuleBase):\n",
    "\n",
    "    def __init__(self, mlp_cfg: dict, envs: gym.vector.SyncVectorEnv):\n",
    "        inp_channels = math.prod(envs.single_observation_space.shape)\n",
    "        out_channels = envs.single_action_space.n\n",
    "        super().__init__(mlp_cfg, inp_channels, out_channels)\n",
    "\n",
    "\n",
    "class PPOCritic(PPOModuleBase):\n",
    "\n",
    "    def __init__(self, mlp_cfg: dict, envs: gym.vector.SyncVectorEnv):\n",
    "        inp_channels = math.prod(envs.single_observation_space.shape)\n",
    "        out_channels = 1\n",
    "        super().__init__(mlp_cfg, inp_channels, out_channels)\n",
    "\n",
    "\n",
    "class PPOModel(nn.Module):\n",
    "\n",
    "    def __init__(self, envs: gym.vector.SyncVectorEnv, actor_cfg: dict, critic_cfg: dict, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.actor: PPOActor = PPOActor(**actor_cfg, envs=envs)\n",
    "        self.critic: PPOCritic = PPOCritic(**critic_cfg, envs=envs)\n",
    "\n",
    "    def get_action(self, obs: torch.Tensor, act: Optional[torch.Tensor] = None, greedy: bool = False):\n",
    "        act_logits = self.actor(obs)\n",
    "        if greedy:\n",
    "            probs = F.softmax(act_logits, dim=-1)\n",
    "            return torch.argmax(probs, dim=-1)\n",
    "        else:\n",
    "            dist = Categorical(logits=act_logits)\n",
    "            if act is None:\n",
    "                act = dist.sample()\n",
    "            return act, dist.log_prob(act), dist.entropy()\n",
    "\n",
    "    def get_value(self, obs: torch.Tensor):\n",
    "        return self.critic(obs)\n",
    "\n",
    "    def forward(self, obs: torch.Tensor, act: torch.Tensor = None):\n",
    "        act, log_prob, entropy = self.get_action(obs, act)\n",
    "        val = self.get_value(obs)\n",
    "        return act, log_prob, entropy, val\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def estimate_returns_and_advantages(\n",
    "            self,\n",
    "            rewards: torch.Tensor,\n",
    "            values: torch.Tensor,\n",
    "            dones: torch.Tensor,\n",
    "            next_obs: torch.Tensor,\n",
    "            next_done: torch.Tensor,\n",
    "            num_steps: int,\n",
    "            gamma: float,\n",
    "            gae_lambda: float):\n",
    "        next_values = self.get_value(next_obs).reshape(1, -1)\n",
    "        advantages = torch.zeros_like(rewards)\n",
    "        last_gae_lam = 0\n",
    "        for t in reversed(range(num_steps)):\n",
    "            if t == num_steps - 1:\n",
    "                next_non_terminal = torch.logical_not(next_done)\n",
    "            else:\n",
    "                next_non_terminal = torch.logical_not(dones[t + 1])\n",
    "                next_values = values[t + 1]\n",
    "            delta = rewards[t] + gamma * next_values * next_non_terminal - values[t]\n",
    "            advantages[t] = last_gae_lam = delta + gamma * gae_lambda * next_non_terminal * last_gae_lam\n",
    "        returns = advantages + values\n",
    "        return returns, advantages\n",
    "\n",
    "\n",
    "class PPOLightning(LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            env_cfg: dict,\n",
    "            data_cfg: dict,\n",
    "            ppo_cfg: dict,\n",
    "            vf_coef: float = 1.0,\n",
    "            ent_coef: float = 0.0,\n",
    "            clip_coef: float = 0.2,\n",
    "            clip_vloss: bool = False,\n",
    "            learning_rate: float = 1e-3,\n",
    "            normalize_advantages: bool = False,\n",
    "            gamma: float = 0.99,\n",
    "            gae_lambda: float = 0.95,\n",
    "            seed=42,\n",
    "            log_root='',\n",
    "            **torchmetrics_kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.env_cfg = env_cfg\n",
    "        self.data_cfg = data_cfg\n",
    "        self.ppo_cfg = ppo_cfg\n",
    "\n",
    "        self.envs = self._load_env(env_cfg, data_cfg, self.hparams.log_root, self.hparams.seed)\n",
    "        self.replay_buffer = self._load_replay_buffer(data_cfg)\n",
    "        self.ppo_model: PPOModel = PPOModel(envs=self.envs, **ppo_cfg)\n",
    "\n",
    "        self.avg_pg_loss = MeanMetric(**torchmetrics_kwargs)\n",
    "        self.avg_value_loss = MeanMetric(**torchmetrics_kwargs)\n",
    "        self.avg_ent_loss = MeanMetric(**torchmetrics_kwargs)\n",
    "\n",
    "        self.vf_coef = vf_coef\n",
    "        self.ent_coef = ent_coef\n",
    "        self.clip_coef = clip_coef\n",
    "        self.clip_vloss = clip_vloss\n",
    "        self.normalize_advantages = normalize_advantages\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "\n",
    "    def forward(self, obs: torch.Tensor, act: Optional[torch.Tensor] = None):\n",
    "        return self.ppo_model(obs, act)\n",
    "\n",
    "    def on_fit_start(self):\n",
    "        num_steps = self.data_cfg.get('total_timestep_n', 1000)\n",
    "\n",
    "        self.observation_shape = self.envs.single_observation_space.shape\n",
    "        self.action_shape = self.envs.single_action_space.shape\n",
    "\n",
    "        self.observations_buffer = torch.zeros((num_steps, self.envs.num_envs) + self.observation_shape,\n",
    "                                               device=self.device)\n",
    "        self.actions_buffer = torch.zeros((num_steps, self.envs.num_envs) + self.action_shape, device=self.device)\n",
    "        self.log_probs_buffer = torch.zeros((num_steps, self.envs.num_envs), device=self.device)\n",
    "        self.rewards_buffer = torch.zeros((num_steps, self.envs.num_envs), device=self.device)\n",
    "        self.dones_buffer = torch.zeros((num_steps, self.envs.num_envs), device=self.device)\n",
    "        self.values_buffer = torch.zeros((num_steps, self.envs.num_envs), device=self.device)\n",
    "\n",
    "        self.on_train_epoch_start()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def on_train_epoch_start(self):\n",
    "        self.eval()\n",
    "\n",
    "        env_eps = 0\n",
    "        env_rew = 0\n",
    "        env_eps_len = 0\n",
    "\n",
    "        num_steps = self.data_cfg.get('total_timestep_n', 1000)\n",
    "\n",
    "        next_observations = torch.tensor(self.envs.reset()[0], device=self.device)\n",
    "        next_dones = torch.zeros(self.envs.num_envs, device=self.device)\n",
    "        for step in range(0, num_steps):\n",
    "            self.observations_buffer[step] = next_observations\n",
    "            self.dones_buffer[step] = next_dones\n",
    "\n",
    "            actions, log_probs, _, values = self(next_observations)\n",
    "            self.values_buffer[step] = values.flatten()\n",
    "            self.actions_buffer[step] = actions\n",
    "            self.log_probs_buffer[step] = log_probs\n",
    "\n",
    "            next_observations, rewards, dones, truncateds, info = self.envs.step(actions.cpu().numpy())\n",
    "            dones = torch.logical_or(torch.tensor(dones), torch.tensor(truncateds))\n",
    "            self.rewards_buffer[step] = torch.tensor(rewards.astype(np.float32), device=self.device).view(-1)\n",
    "\n",
    "            next_observations = torch.tensor(next_observations, device=self.device)\n",
    "            next_dones = dones.to(self.device)\n",
    "\n",
    "            if 'final_info' in info:\n",
    "                for i, agent_final_info in enumerate(info['final_info']):\n",
    "                    if agent_final_info is not None and 'episode' in agent_final_info:\n",
    "                        env_eps += 1\n",
    "                        env_rew += agent_final_info['episode']['r'][0]\n",
    "                        env_eps_len += agent_final_info['episode']['l'][0]\n",
    "\n",
    "        self.logger.log_metrics({\"env/mean_episodes_reward\": env_rew / (env_eps + 1e-8),\n",
    "                                 \"env/mean_episodes_length\": env_eps_len / (env_eps + 1e-8)},\n",
    "                                self.current_epoch)\n",
    "\n",
    "        returns, advantages = self.ppo_model.estimate_returns_and_advantages(self.rewards_buffer, self.values_buffer,\n",
    "                                                                             self.dones_buffer, next_observations,\n",
    "                                                                             next_dones, num_steps, self.gamma,\n",
    "                                                                             self.gae_lambda)\n",
    "\n",
    "        obs_data = self.observations_buffer.reshape((-1,) + self.observation_shape)\n",
    "        log_prob_data = self.log_probs_buffer.reshape(-1)\n",
    "        act_data = self.actions_buffer.reshape((-1,) + self.action_shape)\n",
    "        adv_data = advantages.reshape(-1)\n",
    "        ret_data = returns.reshape(-1)\n",
    "        val_data = self.values_buffer.reshape(-1)\n",
    "        for obs, log_prob, act, adv, ret, val in zip(obs_data, log_prob_data, act_data, adv_data, ret_data, val_data):\n",
    "            self.replay_buffer.append(Experience(\n",
    "                observations=obs,\n",
    "                actions=act,\n",
    "                values=val,\n",
    "                returns=ret,\n",
    "                advantages=adv,\n",
    "                log_probs=log_prob))\n",
    "\n",
    "        self.train()\n",
    "\n",
    "    def training_step(self, batch: dict[str, torch.Tensor]):\n",
    "        _, new_log_prob, entropy, new_value = self(batch['observations'], batch['actions'])\n",
    "        log_ratio = new_log_prob - batch['log_probs']\n",
    "        ratio = torch.exp(log_ratio)\n",
    "\n",
    "        # policy loss\n",
    "        advantages = batch['advantages']\n",
    "        if self.normalize_advantages:\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        p_loss = policy_loss(batch['advantages'], ratio, self.clip_coef)\n",
    "\n",
    "        # value loss\n",
    "        v_loss = value_loss(new_value, batch['values'], batch['returns'], self.clip_coef, self.clip_vloss, self.vf_coef)\n",
    "\n",
    "        # entropy loss\n",
    "        e_loss = entropy_loss(entropy, self.ent_coef)\n",
    "\n",
    "        # update metrics\n",
    "        self.avg_pg_loss.update(p_loss)\n",
    "        self.avg_value_loss.update(v_loss)\n",
    "        self.avg_ent_loss.update(e_loss)\n",
    "\n",
    "        self.log_dict({'p_loss': p_loss.item(), 'v_loss': v_loss.item(), 'e_loss': e_loss.item()},\n",
    "                      sync_dist=True, prog_bar=True, on_epoch=True)\n",
    "\n",
    "        return p_loss + e_loss + v_loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.replay_buffer.clear()\n",
    "\n",
    "        self.logger.log_metrics(\n",
    "            {\n",
    "                \"Loss/policy_loss\": self.avg_pg_loss.compute(),\n",
    "                \"Loss/value_loss\": self.avg_value_loss.compute(),\n",
    "                \"Loss/entropy_loss\": self.avg_ent_loss.compute(),\n",
    "            },\n",
    "            self.global_step)\n",
    "        self.reset_metrics()\n",
    "\n",
    "    def on_fit_end(self):\n",
    "        self.envs.close()\n",
    "\n",
    "    def reset_metrics(self):\n",
    "        self.avg_pg_loss.reset()\n",
    "        self.avg_value_loss.reset()\n",
    "        self.avg_ent_loss.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate, eps=1e-4)\n",
    "\n",
    "    @classmethod\n",
    "    def _load_env(cls, env_cfg: dict, data_cfg: dict, log_root: str = None, seed: int = 42) -> gym.vector.SyncVectorEnv:\n",
    "        def make_env(env_type, env_cfg: dict, idx: int):\n",
    "            def thunk():\n",
    "                env = gym.make(env_type, **env_cfg)\n",
    "                env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "                if idx == 0 and log_root is not None:\n",
    "                    env = gym.wrappers.RecordVideo(\n",
    "                        env, os.path.join(log_root, 'videos'), disable_logger=True)\n",
    "                env.action_space.seed(seed)\n",
    "                env.observation_space.seed(seed)\n",
    "                return env\n",
    "            return thunk\n",
    "\n",
    "        batch_size = data_cfg.get('batch_size', 1)\n",
    "        env_type = env_cfg.pop('type')\n",
    "        envs = gym.vector.SyncVectorEnv([make_env(env_type, env_cfg, _) for _ in range(batch_size)])\n",
    "        return envs\n",
    "\n",
    "    @classmethod\n",
    "    def _load_replay_buffer(cls, data_cfg: dict) -> ReplayBuffer:\n",
    "        total_timestep_n = data_cfg.get('total_timestep_n', 1000)\n",
    "        buffer = ReplayBuffer(total_timestep_n)\n",
    "        return buffer\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        dataset = RLDataset(self.replay_buffer, self.data_cfg['sample_timestep_n'])\n",
    "        return DataLoader(dataset, batch_size=self.data_cfg['batch_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = 42\n",
    "# env_type = 'CartPole-v1'\n",
    "# num_envs = 4\n",
    "# num_steps = 2048\n",
    "# total_timesteps = 500000\n",
    "# num_updates = total_timesteps // (num_envs * num_steps)\n",
    "\n",
    "# seed_everything(seed)\n",
    "\n",
    "# tb_logger = pl_loggers.TensorBoardLogger(save_dir=os.path.join('logs'), name=env_type)\n",
    "\n",
    "# envs = PPOLightning._load_env({'type': env_type, \"render_mode\": \"rgb_array\"},\n",
    "#                                {'batch_size': num_envs},\n",
    "#                                log_root=tb_logger.log_dir,\n",
    "#                                seed=seed)\n",
    "\n",
    "# ppo_model = PPOModel(\n",
    "#     envs=envs,\n",
    "#     actor_cfg=dict(\n",
    "#         mlp_cfg=dict(\n",
    "#             channels=[64, 64],\n",
    "#             use_layer_norm=False,\n",
    "#             act_func='ReLU')),\n",
    "#     critic_cfg=dict(\n",
    "#         mlp_cfg=dict(\n",
    "#             channels=[64, 64],\n",
    "#             use_layer_norm=False,\n",
    "#             act_func='ReLU')))\n",
    "\n",
    "# optimizer = torch.optim.Adam(ppo_model.parameters(), lr=1e-4)\n",
    "\n",
    "# for name, param in ppo_model.named_parameters():\n",
    "#     print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorboard import notebook\n",
    "\n",
    "\n",
    "# local_rew = 0.0\n",
    "# local_ep_len = 0.0\n",
    "# local_num_episodes = 0.0\n",
    "# global_steps = 0.0\n",
    "# update_epochs = 10\n",
    "# vf_coef: float = 1.0\n",
    "# ent_coef: float = 0.0\n",
    "# clip_coef: float = 0.2\n",
    "# clip_vloss: bool = False\n",
    "# gamma = 0.99\n",
    "# gae_lambda = 0.95\n",
    "\n",
    "# obs = torch.zeros((num_steps, num_envs) + envs.single_observation_space.shape)\n",
    "# actions = torch.zeros((num_steps, num_envs) + envs.single_action_space.shape)\n",
    "# logprobs = torch.zeros((num_steps, num_envs))\n",
    "# rewards = torch.zeros((num_steps, num_envs))\n",
    "# dones = torch.zeros((num_steps, num_envs))\n",
    "# values = torch.zeros((num_steps, num_envs))\n",
    "\n",
    "# next_obs = torch.tensor(envs.reset(seed=seed)[0])\n",
    "# next_done = torch.zeros(num_envs)\n",
    "# for update in range(1, num_updates + 1):\n",
    "#     for step in tqdm_notebook.tqdm_notebook(\n",
    "#             range(num_steps), \n",
    "#             desc=f'Update num: {update}/{num_steps}, generating trajections'):\n",
    "#         global_steps += num_envs\n",
    "#         obs[step] = next_obs\n",
    "#         dones[step] = next_done\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             ppo_model.eval()\n",
    "#             action, logprob, _, value = ppo_model(next_obs)\n",
    "#             values[step] = value.flatten()\n",
    "#         actions[step] = action\n",
    "#         logprobs[step] = logprob\n",
    "\n",
    "#         next_obs, reward, done, truncated, info = envs.step(action.cpu().numpy())\n",
    "#         done = torch.logical_or(torch.tensor(done), torch.tensor(truncated))\n",
    "#         rewards[step] = torch.tensor(reward).view(-1)\n",
    "#         next_obs = torch.tensor(next_obs)\n",
    "#         next_done = done\n",
    "\n",
    "#         if 'final_info' in info:\n",
    "#             for i, agent_final_info in enumerate(info['final_info']):\n",
    "#                 if agent_final_info is not None and \"episode\" in agent_final_info:\n",
    "#                     local_num_episodes += 1\n",
    "#                     local_rew += agent_final_info['episode']['r'][0]\n",
    "#                     local_ep_len += agent_final_info['episode']['l'][0]\n",
    "\n",
    "#     if local_num_episodes != 0:\n",
    "#         tb_logger.log_metrics({\n",
    "#             'rewards': local_rew / local_num_episodes, \n",
    "#             'episode_lengths': local_ep_len / local_num_episodes,\n",
    "#         }, step=global_steps)\n",
    "\n",
    "#     local_rew = 0.0\n",
    "#     local_ep_len = 0.0\n",
    "#     local_num_episodes = 0.0\n",
    "#     returns, advantages = ppo_model.estimate_returns_and_advantages(\n",
    "#         rewards, values, dones, next_obs, next_done, num_steps, gamma, gae_lambda\n",
    "#     )\n",
    "\n",
    "#     curr_data = {\n",
    "#         'obs': obs.reshape((-1,) + envs.single_observation_space.shape),\n",
    "#         'logprobs': logprobs.reshape(-1),\n",
    "#         'actions': actions.reshape((-1,) + envs.single_action_space.shape),\n",
    "#         'advantages': advantages.reshape(-1),\n",
    "#         'returns': returns.reshape(-1),\n",
    "#         'values': values.reshape(-1),\n",
    "#     }\n",
    "\n",
    "#     if update == 1:\n",
    "#         with open('train_ppo_ipynb.pkl', 'wb') as fp:\n",
    "#             import pickle as pkl\n",
    "#             pkl.dump(curr_data, fp)\n",
    "\n",
    "#     random_sampler = torch.utils.data.RandomSampler(list(range(curr_data['obs'].shape[0])))\n",
    "#     sampler = torch.utils.data.BatchSampler(random_sampler, batch_size=num_envs, drop_last=False)\n",
    "\n",
    "#     ppo_model.train()\n",
    "#     for epoch in range(update_epochs):\n",
    "#         per_epoch_p_loss = 0.0\n",
    "#         per_epoch_v_loss = 0.0\n",
    "#         per_epoch_e_loss = 0.0\n",
    "#         for batch_idxs in tqdm_notebook.tqdm_notebook(\n",
    "#                 sampler,\n",
    "#                 desc=f'Epoch num: {epoch + 1}/{update_epochs}, updating model',\n",
    "#                 leave=False):\n",
    "#             _, newlogprob, entropy, newvalue = ppo_model(curr_data['obs'][batch_idxs],\n",
    "#                                                          curr_data['actions'][batch_idxs].long())\n",
    "#             logratio = newlogprob - curr_data['logprobs'][batch_idxs]\n",
    "#             ratio = logratio.exp()\n",
    "\n",
    "#             advantages = curr_data['advantages'][batch_idxs]\n",
    "\n",
    "#             p_loss = policy_loss(advantages, ratio, clip_coef)\n",
    "#             v_loss = value_loss(newvalue, curr_data['values'][batch_idxs], curr_data['returns'][batch_idxs],\n",
    "#                                 clip_coef, clip_vloss, vf_coef)\n",
    "#             e_loss = entropy_loss(entropy, ent_coef)\n",
    "\n",
    "#             loss = p_loss + v_loss + e_loss\n",
    "#             per_epoch_p_loss += p_loss.item()\n",
    "#             per_epoch_v_loss += v_loss.item()\n",
    "#             per_epoch_e_loss += e_loss.item()\n",
    "\n",
    "#             optimizer.zero_grad(set_to_none=True)\n",
    "#             loss.backward()\n",
    "#             nn.utils.clip_grad_norm_(ppo_model.parameters(), 0.5)\n",
    "#             optimizer.step()\n",
    "\n",
    "#         tb_logger.log_metrics({\n",
    "#             'losses/p_loss': per_epoch_p_loss / len(sampler),\n",
    "#             'losses/v_loss': per_epoch_v_loss / len(sampler),\n",
    "#             'losses/e_loss': per_epoch_e_loss / len(sampler),\n",
    "#         }, step=global_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPOLightning(\n",
      "  (ppo_model): PPOModel(\n",
      "    (actor): PPOActor(\n",
      "      (net): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "          (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Linear(in_features=64, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (critic): PPOCritic(\n",
      "      (net): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "          (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Linear(in_features=64, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (avg_pg_loss): MeanMetric()\n",
      "  (avg_value_loss): MeanMetric()\n",
      "  (avg_ent_loss): MeanMetric()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name           | Type       | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | ppo_model      | PPOModel   | 9.7 K  | train\n",
      "1 | avg_pg_loss    | MeanMetric | 0      | train\n",
      "2 | avg_value_loss | MeanMetric | 0      | train\n",
      "3 | avg_ent_loss   | MeanMetric | 0      | train\n",
      "------------------------------------------------------\n",
      "9.7 K     Trainable params\n",
      "0         Non-trainable params\n",
      "9.7 K     Total params\n",
      "0.039     Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/Users/yuhang09/miniforge3/envs/ppo/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/yuhang09/miniforge3/envs/ppo/lib/python3.9/site-packages/lightning/pytorch/utilities/data.py:123: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5435638121546079921595132342b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/ppo/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m~/miniforge3/envs/ppo/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:575\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    569\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    571\u001b[0m     ckpt_path,\n\u001b[1;32m    572\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    574\u001b[0m )\n\u001b[0;32m--> 575\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[0;32m~/miniforge3/envs/ppo/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:982\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 982\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ppo/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:1026\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ppo/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:215\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 215\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_advance_start\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance()\n",
      "File \u001b[0;32m~/miniforge3/envs/ppo/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:438\u001b[0m, in \u001b[0;36m_FitLoop.on_advance_start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    437\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(trainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_train_epoch_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 438\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mon_train_epoch_start\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch_progress\u001b[38;5;241m.\u001b[39mincrement_started()\n",
      "File \u001b[0;32m~/miniforge3/envs/ppo/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py:171\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 171\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ppo/lib/python3.9/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 180\u001b[0m, in \u001b[0;36mPPOLightning.on_train_epoch_start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_probs_buffer[step] \u001b[38;5;241m=\u001b[39m log_probs\n\u001b[0;32m--> 180\u001b[0m next_observations, rewards, dones, truncateds, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs\u001b[38;5;241m.\u001b[39mstep(\u001b[43mactions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m    181\u001b[0m dones \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlogical_or(torch\u001b[38;5;241m.\u001b[39mtensor(dones), torch\u001b[38;5;241m.\u001b[39mtensor(truncateds))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n\u001b[1;32m     36\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m16\u001b[39m, logger\u001b[38;5;241m=\u001b[39mtb_logger)\n\u001b[0;32m---> 37\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/ppo/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:539\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 539\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/ppo/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[1;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     67\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "env_type = 'CartPole-v1'\n",
    "\n",
    "model_cfg = dict(\n",
    "    env_cfg=dict(\n",
    "        type=env_type,\n",
    "        render_mode=\"rgb_array\"),\n",
    "    data_cfg=dict(\n",
    "        total_timestep_n=2048,\n",
    "        sample_timestep_n=1024,\n",
    "        batch_size=8),\n",
    "    ppo_cfg=dict(\n",
    "        actor_cfg=dict(\n",
    "            mlp_cfg=dict(\n",
    "                channels=[64, 64],\n",
    "                use_layer_norm=True,\n",
    "                act_func='ReLU')),\n",
    "        critic_cfg=dict(\n",
    "            mlp_cfg=dict(\n",
    "                channels=[64, 64],\n",
    "                use_layer_norm=True,\n",
    "                act_func='ReLU'))),\n",
    "    vf_coef=1.0,\n",
    "    ent_coef=0.0,\n",
    "    clip_coef=0.2,\n",
    "    clip_vloss=True,\n",
    "    normalize_advantages=False,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    learning_rate=1e-4,\n",
    ")\n",
    "\n",
    "tb_logger = pl_loggers.TensorBoardLogger(save_dir=os.path.join('logs'), name=env_type)\n",
    "model = PPOLightning(**model_cfg, log_root=tb_logger.log_dir)\n",
    "print(model)\n",
    "\n",
    "trainer = Trainer(max_epochs=2 ** 16, logger=tb_logger)\n",
    "trainer.fit(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
