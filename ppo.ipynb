{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "from collections import namedtuple, deque\n",
    "import math\n",
    "import random\n",
    "import logging\n",
    "\n",
    "import tqdm\n",
    "import tqdm.notebook as tqdm_notebook\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataset import IterableDataset\n",
    "from torchmetrics import MeanMetric\n",
    "\n",
    "from lightning.pytorch import LightningModule, loggers as pl_loggers\n",
    "from lightning.pytorch.trainer import Trainer\n",
    "from lightning.pytorch import seed_everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('lightning.pytorch')\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple('Experience', ['observations', 'actions', 'values', 'returns', 'advantages', 'log_probs'])\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, capacity: int = 1000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, sample_size: int = 1):\n",
    "        replace = False if sample_size < len(self) else True\n",
    "        indices = np.random.choice(len(self), sample_size, replace=replace)\n",
    "        # collate experiences\n",
    "        for idx in indices:\n",
    "            yield {key: getattr(self.buffer[idx], key) for key in Experience._fields}\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "\n",
    "\n",
    "class RLDataset(IterableDataset):\n",
    "\n",
    "    def __init__(self, buffer: ReplayBuffer, sample_step_num: int = 1):\n",
    "        self.buffer = buffer\n",
    "        self.sample_step_num = sample_step_num\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.sample_step_num\n",
    "\n",
    "    def __iter__(self):\n",
    "        for data in self.buffer.sample(self.sample_step_num):\n",
    "            yield data\n",
    "\n",
    "\n",
    "class FakeDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_loss_factory(clip_coef: float):\n",
    "    def policy_loss(advantages: torch.Tensor, ratio: torch.Tensor):\n",
    "        p_loss1 = -advantages * ratio\n",
    "        p_loss2 = -advantages * torch.clamp(ratio, 1 - clip_coef, 1 + clip_coef)\n",
    "        return torch.max(p_loss1, p_loss2).mean()\n",
    "    return policy_loss\n",
    "\n",
    "\n",
    "def value_loss_factory(clip_coef: float, clip_vloss: bool, vf_coef: float):\n",
    "    def value_loss(new_values: torch.Tensor, old_values: torch.Tensor, returns: torch.Tensor):\n",
    "        new_values = new_values.view(-1)\n",
    "        if not clip_vloss:\n",
    "            values_pred = new_values\n",
    "        else:\n",
    "            values_pred = old_values + torch.clamp(new_values - old_values, -clip_coef, clip_coef)\n",
    "        return vf_coef * F.mse_loss(values_pred, returns)\n",
    "    return value_loss\n",
    "\n",
    "\n",
    "def entropy_loss_factory(ent_coef: float):\n",
    "    def entropy_loss(entropy: torch.Tensor):\n",
    "        return -entropy.mean() * ent_coef\n",
    "    return entropy_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from os import truncate\n",
    "\n",
    "\n",
    "class PPOModuleBase(nn.Module):\n",
    "\n",
    "    def __init__(self, mlp_cfg: dict, inp_channels: int, out_channels: int, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = self._create_mlp(mlp_cfg, inp_channels, out_channels)\n",
    "\n",
    "    def _create_mlp(self, mlp_cfg, inp_channels, out_channels):\n",
    "        channels = [inp_channels] + mlp_cfg['channels'] + [out_channels]\n",
    "        use_layer_norm = mlp_cfg.get('use_layer_norm', False)\n",
    "        act_func = mlp_cfg.get('act_func', 'ReLU')\n",
    "\n",
    "        _mlp = nn.Sequential(*[\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_chn, out_chn, bias=True),\n",
    "                nn.LayerNorm(out_chn) if use_layer_norm else nn.Identity(),\n",
    "                getattr(nn, act_func)())\n",
    "            for in_chn, out_chn in zip(channels[:-2], channels[1:-1])\n",
    "        ])\n",
    "        _mlp.append(nn.Linear(channels[-2], channels[-1], bias=True))\n",
    "        return _mlp\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class PPOActor(PPOModuleBase):\n",
    "\n",
    "    def __init__(self, mlp_cfg: dict, envs: gym.vector.SyncVectorEnv):\n",
    "        inp_channels = math.prod(envs.single_observation_space.shape)\n",
    "        out_channels = envs.single_action_space.n\n",
    "        super().__init__(mlp_cfg, inp_channels, out_channels)\n",
    "\n",
    "\n",
    "class PPOCritic(PPOModuleBase):\n",
    "\n",
    "    def __init__(self, mlp_cfg: dict, envs: gym.vector.SyncVectorEnv):\n",
    "        inp_channels = math.prod(envs.single_observation_space.shape)\n",
    "        out_channels = 1\n",
    "        super().__init__(mlp_cfg, inp_channels, out_channels)\n",
    "\n",
    "\n",
    "class PPOModel(nn.Module):\n",
    "\n",
    "    def __init__(self, envs: gym.vector.SyncVectorEnv, actor_cfg: dict, critic_cfg: dict, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.actor: PPOActor = PPOActor(**actor_cfg, envs=envs)\n",
    "        self.critic: PPOCritic = PPOCritic(**critic_cfg, envs=envs)\n",
    "\n",
    "    def get_action(self, obs: torch.Tensor, act: Optional[torch.Tensor] = None, greedy: bool = False):\n",
    "        act_logits = self.actor(obs)\n",
    "        if greedy:\n",
    "            probs = F.softmax(act_logits, dim=-1)\n",
    "            return torch.argmax(probs, dim=-1)\n",
    "        else:\n",
    "            dist = Categorical(logits=act_logits)\n",
    "            if act is None:\n",
    "                act = dist.sample()\n",
    "            return act, dist.log_prob(act), dist.entropy()\n",
    "\n",
    "    def get_value(self, obs: torch.Tensor):\n",
    "        return self.critic(obs)\n",
    "\n",
    "    def forward(self, obs: torch.Tensor, act: torch.Tensor = None, greedy: bool = False):\n",
    "        if greedy:\n",
    "            return self.get_action(obs, act, greedy)\n",
    "        else:\n",
    "            act, log_prob, entropy = self.get_action(obs, act)\n",
    "            val = self.get_value(obs)\n",
    "            return act, log_prob, entropy, val\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def estimate_returns_and_advantages(\n",
    "            self,\n",
    "            rewards: torch.Tensor,\n",
    "            values: torch.Tensor,\n",
    "            dones: torch.Tensor,\n",
    "            next_obs: torch.Tensor,\n",
    "            next_done: torch.Tensor,\n",
    "            num_steps: int,\n",
    "            gamma: float,\n",
    "            gae_lambda: float):\n",
    "        next_values = self.get_value(next_obs).reshape(1, -1)\n",
    "        advantages = torch.zeros_like(rewards)\n",
    "        last_gae_lam = 0\n",
    "        for t in reversed(range(num_steps)):\n",
    "            if t == num_steps - 1:\n",
    "                next_non_terminal = torch.logical_not(next_done)\n",
    "            else:\n",
    "                next_non_terminal = torch.logical_not(dones[t + 1])\n",
    "                next_values = values[t + 1]\n",
    "            delta = rewards[t] + gamma * next_values * next_non_terminal - values[t]\n",
    "            advantages[t] = last_gae_lam = delta + gamma * gae_lambda * next_non_terminal * last_gae_lam\n",
    "        returns = advantages + values\n",
    "        return returns, advantages\n",
    "\n",
    "\n",
    "class PPOLightning(LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            env_cfg: dict,\n",
    "            data_cfg: dict,\n",
    "            ppo_cfg: dict,\n",
    "            loss_cfg: dict = {\n",
    "                'policy_loss': {'clip_coef': 0.2},\n",
    "                'value_loss': {'clip_coef': 0.2, 'clip_vloss': False, 'vf_coef': 1.0},\n",
    "                'entropy_loss': {'ent_coef': 0.0}},\n",
    "            optim_cfg: dict = {'lr': 1e-4},\n",
    "            running_cfg: dict = {\n",
    "                'seed': 42,\n",
    "                'log_root': '',\n",
    "                'gamma': 0.99,\n",
    "                'gae_lambda': 0.95,\n",
    "                'normalize_advantages': False,\n",
    "                'update_interval': 1,\n",
    "                'update_steps': 10},\n",
    "            **torchmetrics_kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self._load_loss(loss_cfg)\n",
    "        self.train_envs = self._load_env(env_cfg, data_cfg, \n",
    "                                         os.path.join(self.hparams.running_cfg.get('log_root', ''), 'video', 'train'),\n",
    "                                         self.hparams.running_cfg.get('seed', 42))\n",
    "        self.val_env = self._load_env(self.hparams.env_cfg, {'batch_size': 1},\n",
    "                                      os.path.join(self.hparams.running_cfg.get('log_root', ''), 'video', 'val'),\n",
    "                                      self.hparams.running_cfg.get('seed', 42))\n",
    "\n",
    "        self.replay_buffer = self._load_replay_buffer(data_cfg)\n",
    "        self.ppo_model: PPOModel = PPOModel(envs=self.train_envs, **ppo_cfg)\n",
    "\n",
    "        self.avg_pg_loss = MeanMetric(**torchmetrics_kwargs)\n",
    "        self.avg_value_loss = MeanMetric(**torchmetrics_kwargs)\n",
    "        self.avg_ent_loss = MeanMetric(**torchmetrics_kwargs)\n",
    "\n",
    "        self._reset_data = False\n",
    "\n",
    "    def forward(self, obs: torch.Tensor, act: Optional[torch.Tensor] = None, *args, **kwargs):\n",
    "        return self.ppo_model(obs, act, *args, **kwargs)\n",
    "\n",
    "    def on_fit_start(self):\n",
    "        num_steps = self.hparams.data_cfg.get('total_timestep_n', 1000)\n",
    "\n",
    "        self.observation_shape = self.train_envs.single_observation_space.shape\n",
    "        self.action_shape = self.train_envs.single_action_space.shape\n",
    "\n",
    "        self.observations_buffer = torch.zeros((num_steps, self.train_envs.num_envs) + self.observation_shape,\n",
    "                                               device=self.device)\n",
    "        self.actions_buffer = torch.zeros((num_steps, self.train_envs.num_envs) + self.action_shape, device=self.device)\n",
    "        self.log_probs_buffer = torch.zeros((num_steps, self.train_envs.num_envs), device=self.device)\n",
    "        self.rewards_buffer = torch.zeros((num_steps, self.train_envs.num_envs), device=self.device)\n",
    "        self.dones_buffer = torch.zeros((num_steps, self.train_envs.num_envs), device=self.device)\n",
    "        self.values_buffer = torch.zeros((num_steps, self.train_envs.num_envs), device=self.device)\n",
    "\n",
    "        self._load_data()\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        if self._reset_data:\n",
    "            self._load_data()\n",
    "            self._reset_data = False\n",
    "\n",
    "    def training_step(self, batch: dict[str, torch.Tensor]):\n",
    "        _, new_log_prob, entropy, new_value = self(batch['observations'], batch['actions'])\n",
    "        log_ratio = new_log_prob - batch['log_probs']\n",
    "        ratio = torch.exp(log_ratio)\n",
    "\n",
    "        # policy loss\n",
    "        advantages = batch['advantages']\n",
    "        if self.hparams.running_cfg.get('normalize_advantages', False):\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        p_loss = self.policy_loss(batch['advantages'], ratio)\n",
    "\n",
    "        # value loss\n",
    "        v_loss = self.value_loss(new_value, batch['values'], batch['returns'])\n",
    "\n",
    "        # entropy loss\n",
    "        e_loss = self.entropy_loss(entropy)\n",
    "\n",
    "        # update metrics\n",
    "        self.avg_pg_loss.update(p_loss)\n",
    "        self.avg_value_loss.update(v_loss)\n",
    "        self.avg_ent_loss.update(e_loss)\n",
    "\n",
    "        return p_loss + e_loss + v_loss\n",
    "\n",
    "    def validation_step(self, *args, **kwargs):\n",
    "        \n",
    "        step = 0\n",
    "        done = False\n",
    "        cumulative_rew = 0\n",
    "        next_obs = torch.tensor(self.val_env.reset(seed=self.hparams.running_cfg.get('seed', 42))[0], device=self.device)\n",
    "        while not done:\n",
    "            action = self(next_obs, greedy=True)\n",
    "            next_obs, reward, done, truncate, _ = self.val_env.step(action.cpu().numpy())\n",
    "            done = done or truncate\n",
    "            cumulative_rew += reward\n",
    "            next_obs = torch.tensor(next_obs, device=self.device)\n",
    "            step += 1\n",
    "\n",
    "        return {'loss': 0, 'val/reward': cumulative_rew, 'val/step': step}\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        update_interval = self.hparams.running_cfg.get('update_interval', 1)\n",
    "        if (self.current_epoch + 1) % update_interval == 0:\n",
    "            self.replay_buffer.clear()\n",
    "            self._reset_data = True\n",
    "\n",
    "        self.logger.log_metrics(\n",
    "            {\n",
    "                \"Loss/policy_loss\": self.avg_pg_loss.compute(),\n",
    "                \"Loss/value_loss\": self.avg_value_loss.compute(),\n",
    "                \"Loss/entropy_loss\": self.avg_ent_loss.compute(),\n",
    "            },\n",
    "            self.global_step)\n",
    "        self.reset_metrics()\n",
    "\n",
    "    def on_fit_end(self):\n",
    "        self.train_envs.close()\n",
    "        self.val_env.close()\n",
    "\n",
    "    def reset_metrics(self):\n",
    "        self.avg_pg_loss.reset()\n",
    "        self.avg_value_loss.reset()\n",
    "        self.avg_ent_loss.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.hparams.optim_cfg.get('lr', 1e-4)\n",
    "        return torch.optim.Adam(self.parameters(), lr=lr, eps=1e-4)\n",
    "\n",
    "    def _load_loss(self, loss_cfg: dict):\n",
    "        self.policy_loss = policy_loss_factory(**loss_cfg['policy_loss'])\n",
    "        self.value_loss = value_loss_factory(**loss_cfg['value_loss'])\n",
    "        self.entropy_loss = entropy_loss_factory(**loss_cfg['entropy_loss'])\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _load_data(self):\n",
    "        self.eval()\n",
    "\n",
    "        env_eps = 0\n",
    "        env_rew = 0\n",
    "        env_eps_len = 0\n",
    "\n",
    "        num_steps = self.hparams.data_cfg.get('total_timestep_n', 1000)\n",
    "\n",
    "        gamma = self.hparams.running_cfg.get('gamma', 0.99)\n",
    "        gae_lambda = self.hparams.running_cfg.get('gae_lambda', 0.95)\n",
    "\n",
    "        next_observations = torch.tensor(self.train_envs.reset()[0], device=self.device)\n",
    "        next_dones = torch.zeros(self.train_envs.num_envs, device=self.device)\n",
    "        for step in range(0, num_steps):\n",
    "            self.observations_buffer[step] = next_observations\n",
    "            self.dones_buffer[step] = next_dones\n",
    "\n",
    "            actions, log_probs, _, values = self(next_observations)\n",
    "            self.values_buffer[step] = values.flatten()\n",
    "            self.actions_buffer[step] = actions\n",
    "            self.log_probs_buffer[step] = log_probs\n",
    "\n",
    "            next_observations, rewards, dones, truncateds, info = self.train_envs.step(actions.cpu().numpy())\n",
    "            dones = torch.logical_or(torch.tensor(dones), torch.tensor(truncateds))\n",
    "            self.rewards_buffer[step] = torch.tensor(rewards.astype(np.float32), device=self.device).view(-1)\n",
    "\n",
    "            next_observations = torch.tensor(next_observations, device=self.device)\n",
    "            next_dones = dones.to(self.device)\n",
    "\n",
    "            episode = info.get('episode', None)\n",
    "            if episode:\n",
    "                for r, l in zip(episode['r'], episode['l']):\n",
    "                    env_eps += 1\n",
    "                    env_rew += r\n",
    "                    env_eps_len += l\n",
    "\n",
    "        self.logger.log_metrics({\"env/mean_episodes_reward\": env_rew / (env_eps + 1e-8),\n",
    "                                 \"env/mean_episodes_length\": env_eps_len / (env_eps + 1e-8)},\n",
    "                                self.current_epoch)\n",
    "\n",
    "        returns, advantages = self.ppo_model.estimate_returns_and_advantages(self.rewards_buffer, self.values_buffer,\n",
    "                                                                             self.dones_buffer, next_observations,\n",
    "                                                                             next_dones, num_steps, gamma, gae_lambda)\n",
    "\n",
    "        obs_data = self.observations_buffer.reshape((-1,) + self.observation_shape)\n",
    "        log_prob_data = self.log_probs_buffer.reshape(-1)\n",
    "        act_data = self.actions_buffer.reshape((-1,) + self.action_shape)\n",
    "        adv_data = advantages.reshape(-1)\n",
    "        ret_data = returns.reshape(-1)\n",
    "        val_data = self.values_buffer.reshape(-1)\n",
    "        for obs, log_prob, act, adv, ret, val in zip(obs_data, log_prob_data, act_data, adv_data, ret_data, val_data):\n",
    "            self.replay_buffer.append(Experience(\n",
    "                observations=obs,\n",
    "                actions=act,\n",
    "                values=val,\n",
    "                returns=ret,\n",
    "                advantages=adv,\n",
    "                log_probs=log_prob))\n",
    "\n",
    "        self.train()\n",
    "\n",
    "    @classmethod\n",
    "    def _load_env(cls, env_cfg: dict, data_cfg: dict, log_root: str = None, seed: int = 42) -> gym.vector.SyncVectorEnv:\n",
    "        def make_env(env_type, env_cfg: dict, idx: int):\n",
    "            def thunk():\n",
    "                env = gym.make(env_type, **env_cfg)\n",
    "                env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "                if idx == 0 and log_root:\n",
    "                    env = gym.wrappers.RecordVideo(env, log_root, disable_logger=True, )\n",
    "                env.action_space.seed(seed)\n",
    "                env.observation_space.seed(seed)\n",
    "                return env\n",
    "            return thunk\n",
    "\n",
    "        curr_env_cfg = deepcopy(env_cfg)\n",
    "        batch_size = data_cfg.get('batch_size', 1)\n",
    "        env_type = curr_env_cfg.pop('type')\n",
    "        envs = gym.vector.SyncVectorEnv([make_env(env_type, curr_env_cfg, _) for _ in range(batch_size)])\n",
    "        return envs\n",
    "\n",
    "    @classmethod\n",
    "    def _load_replay_buffer(cls, data_cfg: dict) -> ReplayBuffer:\n",
    "        total_timestep_n = data_cfg.get('total_timestep_n', 1000)\n",
    "        buffer = ReplayBuffer(total_timestep_n)\n",
    "        return buffer\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        batch_size = self.hparams.data_cfg.get('batch_size', 1)\n",
    "        sample_timestep_n = max(self.hparams.data_cfg['sample_timestep_n'],\n",
    "                                self.hparams.running_cfg['update_steps'] * batch_size)\n",
    "        dataset = RLDataset(self.replay_buffer, sample_timestep_n)\n",
    "        return DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        dataset = FakeDataset(list([i for i in range(1)]))\n",
    "        return DataLoader(dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_type = 'CartPole-v1'\n",
    "log_root = 'logs'\n",
    "\n",
    "tb_logger = pl_loggers.TensorBoardLogger(save_dir=log_root, name=env_type)\n",
    "\n",
    "model_cfg = dict(\n",
    "    env_cfg=dict(\n",
    "        type=env_type,\n",
    "        render_mode=\"rgb_array\"),\n",
    "    data_cfg=dict(\n",
    "        total_timestep_n=2048,\n",
    "        sample_timestep_n=1024,\n",
    "        batch_size=16),\n",
    "    ppo_cfg=dict(\n",
    "        actor_cfg=dict(\n",
    "            mlp_cfg=dict(\n",
    "                channels=[64, 64],\n",
    "                use_layer_norm=True,\n",
    "                act_func='ReLU')),\n",
    "        critic_cfg=dict(\n",
    "            mlp_cfg=dict(\n",
    "                channels=[64, 64],\n",
    "                use_layer_norm=True,\n",
    "                act_func='ReLU'))),\n",
    "    loss_cfg=dict(\n",
    "        policy_loss=dict(clip_coef=0.2),\n",
    "        value_loss=dict(\n",
    "            clip_coef=0.2,\n",
    "            clip_vloss=False,\n",
    "            vf_coef=1.0),\n",
    "        entropy_loss=dict(ent_coef=0.0)),\n",
    "    optim_cfg=dict(lr=1e-4),\n",
    "    running_cfg=dict(\n",
    "        seed=42,\n",
    "        log_root=tb_logger.log_dir,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        normalize_advantages=False,\n",
    "        update_interval=10,\n",
    "        update_steps=10),\n",
    ")\n",
    "\n",
    "model = PPOLightning(**model_cfg)\n",
    "trainer = Trainer(max_steps=5000, logger=tb_logger, val_check_interval=100, check_val_every_n_epoch=None)\n",
    "trainer.fit(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
