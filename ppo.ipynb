{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from collections import namedtuple, deque\n",
    "import math\n",
    "import random\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import IterableDataset\n",
    "from torchmetrics import MeanMetric\n",
    "\n",
    "from lightning.pytorch import LightningModule\n",
    "from lightning.pytorch.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done', 'log_prob'])\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, capacity: int = 1000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, sample_size: int = 1):\n",
    "        replace = False if sample_size < len(self) else True\n",
    "        indices = np.random.choice(len(self), sample_size, replace=replace)\n",
    "        # collate experiences\n",
    "        for idx in indices:\n",
    "            yield {key: getattr(self.buffer[idx], key) for key in Experience._fields}\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "\n",
    "\n",
    "class RLDataset(IterableDataset):\n",
    "\n",
    "    def __init__(self, buffer: ReplayBuffer, sample_step_num: int = 1):\n",
    "        self.buffer = buffer\n",
    "        self.sample_step_num = sample_step_num\n",
    "\n",
    "    def __iter__(self):\n",
    "        batched_data_iter = self.buffer.sample(self.sample_step_num)\n",
    "        for _ in batched_data_iter:\n",
    "            yield _\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_loss(advantages: torch.Tensor, ratio: torch.Tensor, cliip_coef: float):\n",
    "    p_loss1 = -advantages * ratio\n",
    "    p_loss2 = -advantages * torch.clamp(ratio, 1 - cliip_coef, 1 + cliip_coef)\n",
    "    return torch.max(p_loss1, p_loss2).mean()\n",
    "\n",
    "\n",
    "def value_loss(new_values: torch.Tensor, old_values: torch.Tensor, returns: torch.Tensor, clip_coef: float,\n",
    "               clip_vloss: bool, vf_coef: float):\n",
    "    new_values = new_values.view(-1)\n",
    "    if not clip_vloss:\n",
    "        values_pred = new_values\n",
    "    else:\n",
    "        values_pred = old_values + torch.clamp(new_values - old_values, -clip_coef, clip_coef)\n",
    "    return vf_coef * F.mse_loss(values_pred, returns)\n",
    "\n",
    "\n",
    "def entropy_loss(entropy: torch.Tensor, ent_coef: float):\n",
    "    return -entropy.mean() * ent_coef\n",
    "\n",
    "\n",
    "class Preprocessor(nn.Module):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, batch: dict[str, torch.Tensor]):\n",
    "        pass\n",
    "\n",
    "\n",
    "class PPOModuleBase(nn.Module):\n",
    "\n",
    "    def __init__(self, mlp_cfg: dict, inp_channels: int, out_channels: int, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = self._create_mlp(mlp_cfg, inp_channels, out_channels)\n",
    "\n",
    "    def _create_mlp(self, mlp_cfg, inp_channels, out_channels):\n",
    "        channels = [inp_channels] + mlp_cfg['channels'] + [out_channels]\n",
    "        use_layer_norm = mlp_cfg.get('use_layer_norm', False)\n",
    "        act_func = mlp_cfg.get('act_func', 'ReLU')\n",
    "\n",
    "        _mlp = nn.Sequential(*[\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_chn, out_chn, bias=False),\n",
    "                nn.LayerNorm(out_chn) if use_layer_norm else nn.Identity(),\n",
    "                getattr(nn, act_func)())\n",
    "            for in_chn, out_chn in zip(channels[:-1], channels[1:])\n",
    "        ])\n",
    "        return _mlp\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class PPOActor(PPOModuleBase):\n",
    "\n",
    "    def __init__(self, mlp_cfg: dict, envs: gym.vector.SyncVectorEnv):\n",
    "        inp_channels = math.prod(envs.single_observation_space.shape)\n",
    "        out_channels = envs.single_action_space.n\n",
    "        super().__init__(mlp_cfg, inp_channels, out_channels)\n",
    "\n",
    "\n",
    "class PPOCritic(PPOModuleBase):\n",
    "\n",
    "    def __init__(self, mlp_cfg: dict, envs: gym.vector.SyncVectorEnv):\n",
    "        inp_channels = math.prod(envs.single_observation_space.shape)\n",
    "        out_channels = 1\n",
    "        super().__init__(mlp_cfg, inp_channels, out_channels)\n",
    "\n",
    "\n",
    "class PPOModule(nn.Module):\n",
    "\n",
    "    def __init__(self, env_cfg: dict, actor_cfg: dict, critic_cfg: dict):\n",
    "        super().__init__()\n",
    "\n",
    "        self.env_cfg = env_cfg\n",
    "        self.actor_cfg = actor_cfg\n",
    "        self.critic_cfg = critic_cfg\n",
    "\n",
    "    def _create_env(self, env_cfg: dict) -> gym.vector.SyncVectorEnv:\n",
    "        batch_size = self.data_cfg['batch_size']\n",
    "\n",
    "        type = env_cfg.pop('type')\n",
    "        envs = gym.vector.SyncVectorEnv([gym.make(type, **env_cfg) for _ in range(batch_size)])\n",
    "\n",
    "        return envs\n",
    "\n",
    "\n",
    "\n",
    "class PPO(LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            env_cfg: dict,\n",
    "            preprocessor_cfg: dict,\n",
    "            actor_cfg: dict,\n",
    "            critic_cfg: dict,\n",
    "            data_cfg: dict,\n",
    "            optim_cfg: dict,\n",
    "            torchmetric_cfg: dict,\n",
    "            normalize_advantages: bool = True,\n",
    "            clip_coef: float = 0.2,\n",
    "            clip_vloss: bool = False,\n",
    "            vf_coef: float = 1.0,\n",
    "            ent_coef: float = 0.0,\n",
    "            \n",
    "            *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.env_cfg = env_cfg\n",
    "        self.actor_cfg = actor_cfg\n",
    "        self.critic_cfg = critic_cfg\n",
    "        self.data_cfg = data_cfg\n",
    "        self.optim_cfg = optim_cfg\n",
    "\n",
    "        self.envs: gym.vector.SyncVectorEnv = self._create_env(env_cfg)\n",
    "\n",
    "        self.preprocessor = Preprocessor(**preprocessor_cfg)\n",
    "        self.actor = PPOActor(**actor_cfg, envs=self.envs)\n",
    "        self.critic = PPOCritic(**critic_cfg, envs=self.envs)\n",
    "\n",
    "        self.avg_p_loss = MeanMetric(**torchmetric_cfg)\n",
    "        self.avg_v_loss = MeanMetric(**torchmetric_cfg)\n",
    "        self.avg_e_loss = MeanMetric(**torchmetric_cfg)\n",
    "\n",
    "        self.normalize_advantages = normalize_advantages\n",
    "        self.clip_coef = clip_coef\n",
    "        self.clip_vloss = clip_vloss\n",
    "        self.vf_coef = vf_coef\n",
    "        self.ent_coef = ent_coef\n",
    "\n",
    "        self.buffer = ReplayBuffer(data_cfg['buffer_size'])\n",
    "\n",
    "    def get_action(self, obs: torch.Tensor, action: Optional[torch.Tensor] = None):\n",
    "        logits = self.actor(obs)\n",
    "        distribution = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = distribution.sample()\n",
    "        return action, distribution.log_prob(action), distribution.entropy()\n",
    "\n",
    "    def get_greedy_action(self, obs: torch.Tensor):\n",
    "        logits = self.actor(obs)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        return torch.argmax(probs, dim=-1)\n",
    "\n",
    "    def get_value(self, obs: torch.Tensor):\n",
    "        return self.critic(obs)\n",
    "\n",
    "    def get_action_and_value(self, obs: torch.Tensor, action: torch.Tensor = None):\n",
    "        action, log_prob, entropy = self.get_action(obs, action)\n",
    "        value = self.get_value(obs)\n",
    "        return action, log_prob, entropy, value\n",
    "\n",
    "    def forward(self, obs: torch.Tensor, action: Optional[torch.Tensor] = None):\n",
    "        return self.get_action_and_value(obs, action)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def estimate_returns_and_advantages(self,\n",
    "                                        rewards: torch.Tensor,\n",
    "                                        values: torch.Tensor,\n",
    "                                        dones: torch.Tensor,\n",
    "                                        next_obs: torch.Tensor,\n",
    "                                        next_done: torch.Tensor,\n",
    "                                        num_steps: int,\n",
    "                                        gamma: float,\n",
    "                                        gae_lambda: float):\n",
    "        next_value = self.get_value(next_obs).reshape(1, -1)\n",
    "        advantages = torch.zeros_like(rewards)\n",
    "        last_gae_lam = 0\n",
    "        for t in reversed(range(num_steps)):\n",
    "            if t == num_steps - 1:\n",
    "                next_non_terminal = torch.logical_not(next_done)\n",
    "                next_values = next_value\n",
    "            else:\n",
    "                next_non_terminal = torch.logical_not(dones[t + 1])\n",
    "                next_values = values[t + 1]\n",
    "            delta = rewards[t] + gamma * next_values * next_non_terminal - values[t]\n",
    "            advantages[t] = last_gae_lam = delta + gamma * gae_lambda * next_non_terminal * last_gae_lam\n",
    "        returns = advantages + values\n",
    "        return returns, advantages\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        num_steps = self.data_cfg['num_steps']\n",
    "        self.eval()\n",
    "\n",
    "        nxt_obs = torch.tensor(self.envs.reset()[0])\n",
    "        nxt_done = torch.zeros(self.envs.num_envs)\n",
    "        for _ in range(0, num_steps):\n",
    "            obs = nxt_obs\n",
    "            done = nxt_done\n",
    "\n",
    "            with torch.no_grad():\n",
    "                action, log_prob, _, value = self.get_action_and_value(nxt_obs)\n",
    "                value = value.flatten()\n",
    "\n",
    "            nxt_obs, reward, done, truncated, info = self.envs.step(action.cpu().numpy())\n",
    "            done = torch.logical_or(torch.tensor(done), torch.tensor(truncated))\n",
    "            reward = torch.tensor(reward, dtype=torch.float32).view(-1)\n",
    "\n",
    "            curr_batch = Experience(obs, action, reward, nxt_obs, done)\n",
    "            self.buffer.append(curr_batch)\n",
    "\n",
    "    def training_step(self, batch: dict[str, torch.Tensor]):\n",
    "        batch = self.preprocessor(batch)\n",
    "\n",
    "        _, new_log_prob, entropy, new_value = self(batch['obs'], batch['actions'].long())\n",
    "        log_ratio = new_log_prob - batch['log_probs']\n",
    "        ratio = log_ratio.exp()\n",
    "\n",
    "        advantages = batch['advantages']\n",
    "        if self.normalize_advantages:\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        # policy loss\n",
    "        p_loss = policy_loss(batch['advantages'], ratio, self.clip_coef)\n",
    "\n",
    "        # value loss\n",
    "        v_loss = value_loss(new_value, batch['values'], batch['returns'], self.clip_coef, self.clip_vloss, self.vf_coef)\n",
    "\n",
    "        # entropy loss\n",
    "        e_loss = entropy_loss(entropy, self.ent_coef)\n",
    "\n",
    "        # update metrics\n",
    "        self.avg_p_loss.update(p_loss)\n",
    "        self.avg_v_loss.update(v_loss)\n",
    "        self.avg_e_loss.update(e_loss)\n",
    "\n",
    "        return p_loss + v_loss + e_loss\n",
    "\n",
    "    def on_train_epoch_end(self, global_step: int):\n",
    "        self.logger.log_metrics(\n",
    "            {\n",
    "                \"Loss/policy_loss\": self.avg_p_loss.compute(),\n",
    "                \"Loss/value_loss\": self.avg_v_loss.compute(),\n",
    "                \"Loss/entropy_loss\": self.avg_e_loss.compute(),\n",
    "            },\n",
    "            global_step)\n",
    "\n",
    "        self.avg_p_loss.reset()\n",
    "        self.avg_v_loss.reset()\n",
    "        self.avg_e_loss.reset()\n",
    "\n",
    "        self.buffer.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim_type = self.optim_cfg.pop('type')\n",
    "        return getattr(torch.optim, optim_type)(self.parameters(), **self.optim_cfg)\n",
    "\n",
    "    def _create_env(self, env_cfg: dict) -> gym.vector.SyncVectorEnv:\n",
    "        batch_size = self.data_cfg['batch_size']\n",
    "\n",
    "        type = env_cfg.pop('type')\n",
    "        envs = gym.vector.SyncVectorEnv([gym.make(type, **env_cfg) for _ in range(batch_size)])\n",
    "\n",
    "        return envs\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        dataset = RLDataset(self.buffer, self.data_cfg['batch_size'])\n",
    "        dataloader = DataLoader(dataset, batch_size=self.data_cfg['batch_size'])\n",
    "        return dataloader\n",
    "\n",
    "    def get_device(self, batch):\n",
    "        return batch[0].device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
